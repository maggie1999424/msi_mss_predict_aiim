{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for checking the strange overfitting-like loss shown by hugging_face training pipeline.\n",
    "WHY DID IT SHOW A VALIDATION LOSS WAY WAY WAY BIGGER THAN TRAIN SET!!!\n",
    "(highly seperate loss of training and validation **from the first epoch utill ends usually means target features in training set and validation are different**. is patient individual effect that obvious??? No way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "threshold2 = 0.5\n",
    "modelname = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/maggie1999424/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub , os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, auc, precision_score, recall_score\n",
    "from transformers import AutoImageProcessor\n",
    "huggingface_hub.login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb005f20ad42469abc20d6573d930377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/129008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f018d0339ee44ba0b1326404a348e802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d169ab38d64183a53679d863f9baae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"kaggle-MSI\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoImageProcessor\n",
    "\n",
    "# checkpoint = \"shi-labs/nat-mini-in1k-224\"\n",
    "# # checkpoint = \"microsoft/swinv2-tiny-patch4-window8-256\"\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(checkpoint, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor, RandomHorizontalFlip, RandomVerticalFlip\n",
    "\n",
    "# normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "# size = (\n",
    "#     image_processor.size[\"shortest_edge\"]\n",
    "#     if \"shortest_edge\" in image_processor.size\n",
    "#     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "# )\n",
    "# _transforms = Compose([RandomResizedCrop(size), RandomVerticalFlip(), RandomHorizontalFlip(), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transforms(examples):\n",
    "#     examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "#     examples[\"image\"]\n",
    "#     return examples\n",
    "\n",
    "\n",
    "# ds = ds.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_val = ds['validation']['image']\n",
    "image_test = ds['test']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"image-classification\", model=\"msi_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = image_val[-500:]\n",
    "output_dict_vv = classifier(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "target = []\n",
    "for i in vv:\n",
    "    for ii in i:\n",
    "        # A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A = [1-ii['score'],ii['score']]\n",
    "    score.append(A)\n",
    "\n",
    "    \n",
    "label = ds['validation']['label'][-500:]\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])\n",
    "\n",
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)\n",
    "val_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "val_loss_BCE= torch.nn.functional.binary_cross_entropy(score[:,1], label)\n",
    "\n",
    "df_val['score'] = score[:,1]\n",
    "df_val['pred'] = df_val['score'] >= threshold    \n",
    "df_val.to_csv(f'{modelname}_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds['train']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dict = []\n",
    "for i in range(0, len(image),16):\n",
    "    k = min(i+16, len(image)-1)\n",
    "    print(i,k)\n",
    "    output_dict.append(classifier(image[i:k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make sure input data are in the alphabet order of label + file name\n",
    "df_train = pd.read_csv('List/Train_list_0_Raw').sort_values(['Y','Path,']).reset_index(drop=True)\n",
    "\n",
    "# for index in range(len(df_train)):\n",
    "#     print(df_train.loc[index])\n",
    "#     print(ds['train'][index])\n",
    "\n",
    "score = []\n",
    "target = []\n",
    "for i in output_dict:\n",
    "    for ii in i:\n",
    "        A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A[1] = ii['score']\n",
    "        else:\n",
    "            A[0] = ii['score']\n",
    "    score.append(A)\n",
    "label = ds['validation']['label']\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])\n",
    "\n",
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)\n",
    "train_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "train_loss_bin= torch.nn.functional.binary_cross_entropy(score[:,1], label)\n",
    "df_train['score'] = score[:,1]\n",
    "df_train['pred'] = df_train['score'] >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = ds['test']['image']\n",
    "len(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dicttest = classifier(image_test[34000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "target = []\n",
    "\n",
    "for i in output_dicttest:\n",
    "    for ii in i:\n",
    "        # A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A = [1-ii['score'],ii['score']]\n",
    "        # else:\n",
    "            # A[0] = ii['score']\n",
    "    score.append(A)\n",
    "label = ds['test']['label'][34000:]\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)\n",
    "test_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "test_loss_bin= torch.nn.functional.binary_cross_entropy(score[:,1], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_loss_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('List/Test_list_Raw').sort_values(['Y','Path']).reset_index(drop=True).loc[:-1]\n",
    "df_test['score'] = score[:,1]\n",
    "df_test['pred'] = df_test['score'] >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc( label, score[:,1]), accuracy_score(label, df_test['pred']), precision_score(label, df_test['pred']), recall_score(label, df_test['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acc slide-wise\n",
    "(Ideally it should be patient-wise, but I did not match slide ID back to patient ID. I do not have the complete patient-slide list either.\n",
    "So the train/test split was based on slide, but not patient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss_df(df):\n",
    "    result_df = pd.DataFrame(None,columns=['slide_ID', 'label','pred','score'],)\n",
    "    pids = df['ID'].unique()\n",
    "    for pid in pids:\n",
    "        df_pid=df[df['ID'] == pid]\n",
    "        label,pred, score = cal_loss_pid(df_pid)\n",
    "        pd.concat([result_df, pd.DataFrame([[pid, pred, score]],columns=['slide_ID','label', 'pred','score'],)])\n",
    "    return result_df\n",
    "    \n",
    "\n",
    "def cal_loss_pid(df, mode = 'majority'):\n",
    "    assert len(df['Y'].unique()) ==1\n",
    "    label = df['Y']\n",
    "    if mode == 'majority':\n",
    "        score = sum(df['pred'])/len(df)\n",
    "    elif mode == 'mean':\n",
    "        score = df['score'].mean()\n",
    "    return label, score >= threshold2 ,score\n",
    "\n",
    "# df_train_result = cal_loss_df(df_train)\n",
    "df_val_result = cal_loss_df(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Acc train: ' ,accuracy_score(df_train_result['label'],df_train_result['pred'])\n",
    "# print('AUC train: ' ,auc(df_train_result['label'],df_train_result['score'])\n",
    "# print('AUPRC train: ' ,precision_score(df_train_result['label'],df_train_result['score'])\n",
    "print(\"\")\n",
    "print('Acc test: ' ,accuracy_score(df_test_result['label'],df_test_result['pred'])\n",
    "print('AUC test: ' ,auc(df_test_result['label'],df_test_result['score'])\n",
    "print('AUPRC test: ' ,precision_score(df_test_result['label'],df_test_result['score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad6a615a1ef1d8e0343a28e4a9fbe60c0c10bbf6792d1c88d31f83b0898db28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
