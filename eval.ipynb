{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for checking the strange overfitting-like loss shown by hugging_face training pipeline.\n",
    "WHY DID IT SHOW A VALIDATION LOSS WAY WAY WAY BIGGER THAN TRAIN SET!!!\n",
    "(highly seperate loss of training and validation **from the first epoch utill ends usually means target features in training set and validation are different**. is patient individual effect that obvious??? No way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "threshold2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/maggie1999424/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub , os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, auc, precision_score\n",
    "from transformers import AutoImageProcessor\n",
    "huggingface_hub.login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892ef111ee204093b5d31b11686c7b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/129008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdade3552c564607908361a7502b2775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3192b8c43f5c499480533292407aff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"kaggle-MSI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"image-classification\", model=\"msi_mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = ds['train']['image']\n",
    "image_val = ds['validation']['image']\n",
    "image_test = ds['test']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict = classifier(image_val)\n",
    "score = []\n",
    "target = []\n",
    "for i in output_dict:\n",
    "    for ii in i:\n",
    "        # A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A = [1-ii['score'],ii['score']]\n",
    "        # else:\n",
    "            # A[0] = ii['score']\n",
    "    score.append(A)\n",
    "\n",
    "    \n",
    "label = ds['validation']['label']\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict =output_dictval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)\n",
    "val_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "val_loss_bin= torch.nn.functional.binary_cross_entropy(score[:,1], label)\n",
    "\n",
    "df_val['score'] = score[:,1]\n",
    "df_val['pred'] = df_val['score'] >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8478188754353956"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_loss_bin\n",
    "precision_score( label, df_val['pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.to_csv('val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds['train']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m16\u001b[39m, \u001b[39mlen\u001b[39m(image)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(i,k)\n\u001b[0;32m----> 5\u001b[0m output_dict\u001b[39m.\u001b[39mappend(classifier(image[i:k]))\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/image_classification.py:106\u001b[0m, in \u001b[0;36mImageClassificationPipeline.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m, List[\u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m    Assign labels to the image(s) passed as inputs.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m        - **score** (`int`) -- The score attributed by the model for that label.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/pipelines/image_classification.py:114\u001b[0m, in \u001b[0;36mImageClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 114\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:808\u001b[0m, in \u001b[0;36mNatForImageClassification.forward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    806\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 808\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnat(\n\u001b[1;32m    809\u001b[0m     pixel_values,\n\u001b[1;32m    810\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    811\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    812\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    813\u001b[0m )\n\u001b[1;32m    815\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    817\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:732\u001b[0m, in \u001b[0;36mNatModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    730\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m--> 732\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    733\u001b[0m     embedding_output,\n\u001b[1;32m    734\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    735\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    736\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    737\u001b[0m )\n\u001b[1;32m    739\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    740\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:590\u001b[0m, in \u001b[0;36mNatEncoder.forward\u001b[0;34m(self, hidden_states, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, return_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m     all_reshaped_hidden_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (reshaped_hidden_state,)\n\u001b[1;32m    589\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels):\n\u001b[0;32m--> 590\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, output_attentions)\n\u001b[1;32m    592\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     hidden_states_before_downsampling \u001b[39m=\u001b[39m layer_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:537\u001b[0m, in \u001b[0;36mNatStage.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m _, height, width, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    536\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m--> 537\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, output_attentions)\n\u001b[1;32m    538\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m hidden_states_before_downsampling \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:480\u001b[0m, in \u001b[0;36mNatLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    476\u001b[0m hidden_states, pad_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaybe_pad(hidden_states, height, width)\n\u001b[1;32m    478\u001b[0m _, height_pad, width_pad, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 480\u001b[0m attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(hidden_states, output_attentions\u001b[39m=\u001b[39moutput_attentions)\n\u001b[1;32m    482\u001b[0m attention_output \u001b[39m=\u001b[39m attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m was_padded \u001b[39m=\u001b[39m pad_values[\u001b[39m3\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m pad_values[\u001b[39m5\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:405\u001b[0m, in \u001b[0;36mNeighborhoodAttentionModule.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    402\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    403\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 405\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(hidden_states, output_attentions)\n\u001b[1;32m    406\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    407\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/transformers/models/nat/modeling_nat.py:334\u001b[0m, in \u001b[0;36mNeighborhoodAttention.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    331\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    333\u001b[0m     query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(hidden_states))\n\u001b[0;32m--> 334\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m    335\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    337\u001b[0m     \u001b[39m# Apply the scale factor before computing attention weights. It's usually more efficient because\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39m# attention weights are typically a bigger tensor compared to query.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[39m# It gives identical results because scalars are commutable in matrix multiplication.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "output_dict = []\n",
    "for i in range(0, len(image),16):\n",
    "    k = min(i+16, len(image)-1)\n",
    "    print(i,k)\n",
    "    output_dict.append(classifier(image[i:k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'score': 0.997480571269989, 'label': '0'},\n",
       "   {'score': 0.0025194701738655567, 'label': '1'}],\n",
       "  [{'score': 0.8542183041572571, 'label': '0'},\n",
       "   {'score': 0.1457817107439041, 'label': '1'}],\n",
       "  [{'score': 0.9898272752761841, 'label': '0'},\n",
       "   {'score': 0.0101727694272995, 'label': '1'}],\n",
       "  [{'score': 0.9999668598175049, 'label': '0'},\n",
       "   {'score': 3.314973582746461e-05, 'label': '1'}],\n",
       "  [{'score': 0.9981741905212402, 'label': '0'},\n",
       "   {'score': 0.0018258440541103482, 'label': '1'}],\n",
       "  [{'score': 0.5466787815093994, 'label': '0'},\n",
       "   {'score': 0.4533211886882782, 'label': '1'}],\n",
       "  [{'score': 0.9999784231185913, 'label': '0'},\n",
       "   {'score': 2.152460911020171e-05, 'label': '1'}],\n",
       "  [{'score': 0.9255451560020447, 'label': '0'},\n",
       "   {'score': 0.0744548887014389, 'label': '1'}],\n",
       "  [{'score': 0.9995865225791931, 'label': '0'},\n",
       "   {'score': 0.0004135090275667608, 'label': '1'}],\n",
       "  [{'score': 0.9997174143791199, 'label': '0'},\n",
       "   {'score': 0.00028256996301934123, 'label': '1'}],\n",
       "  [{'score': 0.9684041142463684, 'label': '1'},\n",
       "   {'score': 0.03159591183066368, 'label': '0'}],\n",
       "  [{'score': 0.9999712705612183, 'label': '0'},\n",
       "   {'score': 2.8778713385690935e-05, 'label': '1'}],\n",
       "  [{'score': 0.7992360591888428, 'label': '0'},\n",
       "   {'score': 0.20076394081115723, 'label': '1'}],\n",
       "  [{'score': 0.8196262121200562, 'label': '0'},\n",
       "   {'score': 0.18037375807762146, 'label': '1'}],\n",
       "  [{'score': 0.9999995231628418, 'label': '0'},\n",
       "   {'score': 5.027617362429737e-07, 'label': '1'}],\n",
       "  [{'score': 0.9976442456245422, 'label': '0'},\n",
       "   {'score': 0.002355762990191579, 'label': '1'}],\n",
       "  [{'score': 0.9999370574951172, 'label': '0'},\n",
       "   {'score': 6.292729813139886e-05, 'label': '1'}]],\n",
       " [[{'score': 0.9999370574951172, 'label': '0'},\n",
       "   {'score': 6.292729813139886e-05, 'label': '1'}]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make sure input data are in the alphabet order of label + file name\n",
    "df_train = pd.read_csv('List/Train_list_0_Raw').sort_values(['Y','Path,']).reset_index(drop=True)\n",
    "\n",
    "# for index in range(len(df_train)):\n",
    "#     print(df_train.loc[index])\n",
    "#     print(ds['train'][index])\n",
    "\n",
    "score = []\n",
    "target = []\n",
    "for i in output_dict:\n",
    "    for ii in i:\n",
    "        A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A[1] = ii['score']\n",
    "        else:\n",
    "            A[0] = ii['score']\n",
    "    score.append(A)\n",
    "label = ds['validation']['label']\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])\n",
    "\n",
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)\n",
    "train_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "train_loss_bin= torch.nn.functional.binary_cross_entropy(score[:,1], label)\n",
    "df_train['score'] = score[:,1]\n",
    "df_train['pred'] = df_train['score'] >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n",
      "16 32\n",
      "32 48\n",
      "48 64\n",
      "64 80\n",
      "80 96\n",
      "96 112\n",
      "112 128\n",
      "128 144\n",
      "144 160\n",
      "160 176\n",
      "176 192\n",
      "192 208\n",
      "208 224\n",
      "224 240\n",
      "240 256\n",
      "256 272\n",
      "272 288\n",
      "288 304\n",
      "304 320\n",
      "320 336\n",
      "336 352\n",
      "352 368\n",
      "368 384\n",
      "384 400\n",
      "400 416\n",
      "416 432\n",
      "432 448\n",
      "448 464\n",
      "464 480\n",
      "480 496\n",
      "496 512\n",
      "512 528\n",
      "528 544\n",
      "544 560\n",
      "560 576\n",
      "576 592\n",
      "592 608\n",
      "608 624\n",
      "624 640\n",
      "640 656\n",
      "656 672\n",
      "672 688\n",
      "688 704\n",
      "704 720\n",
      "720 736\n",
      "736 752\n",
      "752 768\n",
      "768 784\n",
      "784 800\n",
      "800 816\n",
      "816 832\n",
      "832 848\n",
      "848 864\n",
      "864 880\n",
      "880 896\n",
      "896 912\n",
      "912 928\n",
      "928 944\n",
      "944 960\n",
      "960 976\n",
      "976 992\n",
      "992 1008\n",
      "1008 1024\n",
      "1024 1040\n",
      "1040 1056\n",
      "1056 1072\n",
      "1072 1088\n",
      "1088 1104\n",
      "1104 1120\n",
      "1120 1136\n",
      "1136 1152\n",
      "1152 1168\n",
      "1168 1184\n",
      "1184 1200\n",
      "1200 1216\n",
      "1216 1232\n",
      "1232 1248\n",
      "1248 1264\n",
      "1264 1280\n",
      "1280 1296\n",
      "1296 1312\n",
      "1312 1328\n",
      "1328 1344\n",
      "1344 1360\n",
      "1360 1376\n",
      "1376 1392\n",
      "1392 1408\n",
      "1408 1424\n",
      "1424 1440\n",
      "1440 1456\n",
      "1456 1472\n",
      "1472 1488\n",
      "1488 1504\n",
      "1504 1520\n",
      "1520 1536\n",
      "1536 1552\n",
      "1552 1568\n",
      "1568 1584\n",
      "1584 1600\n",
      "1600 1616\n",
      "1616 1632\n",
      "1632 1648\n",
      "1648 1664\n",
      "1664 1680\n",
      "1680 1696\n",
      "1696 1712\n",
      "1712 1728\n",
      "1728 1744\n",
      "1744 1760\n",
      "1760 1776\n",
      "1776 1792\n",
      "1792 1808\n",
      "1808 1824\n",
      "1824 1840\n",
      "1840 1856\n",
      "1856 1872\n",
      "1872 1888\n",
      "1888 1904\n",
      "1904 1920\n",
      "1920 1936\n",
      "1936 1952\n",
      "1952 1968\n",
      "1968 1984\n",
      "1984 2000\n",
      "2000 2016\n",
      "2016 2032\n",
      "2032 2048\n",
      "2048 2064\n",
      "2064 2080\n",
      "2080 2096\n",
      "2096 2112\n",
      "2112 2128\n",
      "2128 2144\n",
      "2144 2160\n",
      "2160 2176\n",
      "2176 2192\n",
      "2192 2208\n",
      "2208 2224\n",
      "2224 2240\n",
      "2240 2256\n",
      "2256 2272\n",
      "2272 2288\n",
      "2288 2304\n",
      "2304 2320\n",
      "2320 2336\n",
      "2336 2352\n",
      "2352 2368\n",
      "2368 2384\n",
      "2384 2400\n",
      "2400 2416\n",
      "2416 2432\n",
      "2432 2448\n",
      "2448 2464\n",
      "2464 2480\n",
      "2480 2496\n",
      "2496 2512\n",
      "2512 2528\n",
      "2528 2544\n",
      "2544 2560\n",
      "2560 2576\n",
      "2576 2592\n",
      "2592 2608\n",
      "2608 2624\n",
      "2624 2640\n",
      "2640 2656\n",
      "2656 2672\n",
      "2672 2688\n",
      "2688 2704\n",
      "2704 2720\n",
      "2720 2736\n",
      "2736 2752\n",
      "2752 2768\n",
      "2768 2784\n",
      "2784 2800\n",
      "2800 2816\n",
      "2816 2832\n",
      "2832 2848\n",
      "2848 2864\n",
      "2864 2880\n",
      "2880 2896\n",
      "2896 2912\n",
      "2912 2928\n",
      "2928 2944\n",
      "2944 2960\n",
      "2960 2976\n",
      "2976 2992\n",
      "2992 3008\n",
      "3008 3024\n",
      "3024 3040\n",
      "3040 3056\n",
      "3056 3072\n",
      "3072 3088\n",
      "3088 3104\n",
      "3104 3120\n",
      "3120 3136\n",
      "3136 3152\n",
      "3152 3168\n",
      "3168 3184\n",
      "3184 3200\n",
      "3200 3216\n",
      "3216 3232\n",
      "3232 3248\n",
      "3248 3264\n",
      "3264 3280\n",
      "3280 3296\n",
      "3296 3312\n",
      "3312 3328\n",
      "3328 3344\n",
      "3344 3360\n",
      "3360 3376\n",
      "3376 3392\n",
      "3392 3408\n",
      "3408 3424\n",
      "3424 3440\n",
      "3440 3456\n",
      "3456 3472\n",
      "3472 3488\n",
      "3488 3504\n",
      "3504 3520\n",
      "3520 3536\n",
      "3536 3552\n",
      "3552 3568\n",
      "3568 3584\n",
      "3584 3600\n",
      "3600 3616\n",
      "3616 3632\n",
      "3632 3648\n",
      "3648 3664\n",
      "3664 3680\n",
      "3680 3696\n",
      "3696 3712\n",
      "3712 3728\n",
      "3728 3744\n",
      "3744 3760\n",
      "3760 3776\n",
      "3776 3792\n",
      "3792 3808\n",
      "3808 3824\n",
      "3824 3840\n",
      "3840 3856\n",
      "3856 3872\n",
      "3872 3888\n",
      "3888 3904\n",
      "3904 3920\n",
      "3920 3936\n",
      "3936 3952\n",
      "3952 3968\n",
      "3968 3984\n",
      "3984 4000\n",
      "4000 4016\n",
      "4016 4032\n",
      "4032 4048\n",
      "4048 4064\n",
      "4064 4080\n",
      "4080 4096\n",
      "4096 4112\n",
      "4112 4128\n",
      "4128 4144\n",
      "4144 4160\n",
      "4160 4176\n",
      "4176 4192\n",
      "4192 4208\n",
      "4208 4224\n",
      "4224 4240\n",
      "4240 4256\n",
      "4256 4272\n",
      "4272 4288\n",
      "4288 4304\n",
      "4304 4320\n",
      "4320 4336\n",
      "4336 4352\n",
      "4352 4368\n",
      "4368 4384\n",
      "4384 4400\n",
      "4400 4416\n",
      "4416 4432\n",
      "4432 4448\n",
      "4448 4464\n",
      "4464 4480\n",
      "4480 4496\n",
      "4496 4512\n",
      "4512 4528\n",
      "4528 4544\n",
      "4544 4560\n",
      "4560 4576\n",
      "4576 4592\n",
      "4592 4608\n",
      "4608 4624\n",
      "4624 4640\n",
      "4640 4656\n",
      "4656 4672\n",
      "4672 4688\n",
      "4688 4704\n",
      "4704 4720\n",
      "4720 4736\n",
      "4736 4752\n",
      "4752 4768\n",
      "4768 4784\n",
      "4784 4800\n",
      "4800 4816\n",
      "4816 4832\n",
      "4832 4848\n",
      "4848 4864\n",
      "4864 4880\n",
      "4880 4896\n",
      "4896 4912\n",
      "4912 4928\n",
      "4928 4944\n",
      "4944 4960\n",
      "4960 4976\n",
      "4976 4992\n",
      "4992 5008\n",
      "5008 5024\n",
      "5024 5040\n",
      "5040 5056\n",
      "5056 5072\n",
      "5072 5088\n",
      "5088 5104\n",
      "5104 5120\n",
      "5120 5136\n",
      "5136 5152\n",
      "5152 5168\n",
      "5168 5184\n",
      "5184 5200\n",
      "5200 5216\n",
      "5216 5232\n",
      "5232 5248\n",
      "5248 5264\n",
      "5264 5280\n",
      "5280 5296\n",
      "5296 5312\n",
      "5312 5328\n",
      "5328 5344\n",
      "5344 5360\n",
      "5360 5376\n",
      "5376 5392\n",
      "5392 5408\n",
      "5408 5424\n",
      "5424 5440\n",
      "5440 5456\n",
      "5456 5472\n",
      "5472 5488\n",
      "5488 5504\n",
      "5504 5520\n",
      "5520 5536\n",
      "5536 5552\n",
      "5552 5568\n",
      "5568 5584\n",
      "5584 5600\n",
      "5600 5616\n",
      "5616 5632\n",
      "5632 5648\n",
      "5648 5664\n",
      "5664 5680\n",
      "5680 5696\n",
      "5696 5712\n",
      "5712 5728\n",
      "5728 5744\n",
      "5744 5760\n",
      "5760 5776\n",
      "5776 5792\n",
      "5792 5808\n",
      "5808 5824\n",
      "5824 5840\n",
      "5840 5856\n",
      "5856 5872\n",
      "5872 5888\n",
      "5888 5904\n",
      "5904 5920\n",
      "5920 5936\n",
      "5936 5952\n",
      "5952 5968\n",
      "5968 5984\n",
      "5984 6000\n",
      "6000 6016\n",
      "6016 6032\n",
      "6032 6048\n",
      "6048 6064\n",
      "6064 6080\n",
      "6080 6096\n",
      "6096 6112\n",
      "6112 6128\n",
      "6128 6144\n",
      "6144 6160\n",
      "6160 6176\n",
      "6176 6192\n",
      "6192 6208\n",
      "6208 6224\n",
      "6224 6240\n",
      "6240 6256\n",
      "6256 6272\n",
      "6272 6288\n",
      "6288 6304\n",
      "6304 6320\n",
      "6320 6336\n",
      "6336 6352\n",
      "6352 6368\n",
      "6368 6384\n",
      "6384 6400\n",
      "6400 6416\n",
      "6416 6432\n",
      "6432 6448\n",
      "6448 6464\n",
      "6464 6480\n",
      "6480 6496\n",
      "6496 6512\n",
      "6512 6528\n",
      "6528 6544\n",
      "6544 6560\n",
      "6560 6576\n",
      "6576 6592\n",
      "6592 6608\n",
      "6608 6624\n",
      "6624 6640\n",
      "6640 6656\n",
      "6656 6672\n",
      "6672 6688\n",
      "6688 6704\n",
      "6704 6720\n",
      "6720 6736\n",
      "6736 6752\n",
      "6752 6768\n",
      "6768 6784\n",
      "6784 6800\n",
      "6800 6816\n",
      "6816 6832\n",
      "6832 6848\n",
      "6848 6864\n",
      "6864 6880\n",
      "6880 6896\n",
      "6896 6912\n",
      "6912 6928\n",
      "6928 6944\n",
      "6944 6960\n",
      "6960 6976\n",
      "6976 6992\n",
      "6992 7008\n",
      "7008 7024\n",
      "7024 7040\n",
      "7040 7056\n",
      "7056 7072\n",
      "7072 7088\n",
      "7088 7104\n",
      "7104 7120\n",
      "7120 7136\n",
      "7136 7152\n",
      "7152 7168\n",
      "7168 7184\n",
      "7184 7200\n",
      "7200 7216\n",
      "7216 7232\n",
      "7232 7248\n",
      "7248 7264\n",
      "7264 7280\n",
      "7280 7296\n",
      "7296 7312\n",
      "7312 7328\n",
      "7328 7344\n",
      "7344 7360\n",
      "7360 7376\n",
      "7376 7392\n",
      "7392 7408\n",
      "7408 7424\n",
      "7424 7440\n",
      "7440 7456\n",
      "7456 7472\n",
      "7472 7488\n",
      "7488 7504\n",
      "7504 7520\n",
      "7520 7536\n",
      "7536 7552\n",
      "7552 7568\n",
      "7568 7584\n",
      "7584 7600\n",
      "7600 7616\n",
      "7616 7632\n",
      "7632 7648\n",
      "7648 7664\n",
      "7664 7680\n",
      "7680 7696\n",
      "7696 7712\n",
      "7712 7728\n",
      "7728 7744\n",
      "7744 7760\n",
      "7760 7776\n",
      "7776 7792\n",
      "7792 7808\n",
      "7808 7824\n",
      "7824 7840\n",
      "7840 7856\n",
      "7856 7872\n",
      "7872 7888\n",
      "7888 7904\n",
      "7904 7920\n",
      "7920 7936\n",
      "7936 7952\n",
      "7952 7968\n",
      "7968 7984\n",
      "7984 8000\n",
      "8000 8016\n",
      "8016 8032\n",
      "8032 8048\n",
      "8048 8064\n",
      "8064 8080\n",
      "8080 8096\n",
      "8096 8112\n",
      "8112 8128\n",
      "8128 8144\n",
      "8144 8160\n",
      "8160 8176\n",
      "8176 8192\n",
      "8192 8208\n",
      "8208 8224\n",
      "8224 8240\n",
      "8240 8256\n",
      "8256 8272\n",
      "8272 8288\n",
      "8288 8304\n",
      "8304 8320\n",
      "8320 8336\n",
      "8336 8352\n",
      "8352 8368\n",
      "8368 8384\n",
      "8384 8400\n",
      "8400 8416\n",
      "8416 8432\n",
      "8432 8448\n",
      "8448 8464\n",
      "8464 8480\n",
      "8480 8496\n",
      "8496 8512\n",
      "8512 8528\n",
      "8528 8544\n",
      "8544 8560\n",
      "8560 8576\n",
      "8576 8592\n",
      "8592 8608\n",
      "8608 8624\n",
      "8624 8640\n",
      "8640 8656\n",
      "8656 8672\n",
      "8672 8688\n",
      "8688 8704\n",
      "8704 8720\n",
      "8720 8736\n",
      "8736 8752\n",
      "8752 8768\n",
      "8768 8784\n",
      "8784 8800\n",
      "8800 8816\n",
      "8816 8832\n",
      "8832 8848\n",
      "8848 8864\n",
      "8864 8880\n",
      "8880 8896\n",
      "8896 8912\n",
      "8912 8928\n",
      "8928 8944\n",
      "8944 8960\n",
      "8960 8976\n",
      "8976 8992\n",
      "8992 9008\n",
      "9008 9024\n",
      "9024 9040\n",
      "9040 9056\n",
      "9056 9072\n",
      "9072 9088\n",
      "9088 9104\n",
      "9104 9120\n",
      "9120 9136\n",
      "9136 9152\n",
      "9152 9168\n",
      "9168 9184\n",
      "9184 9200\n",
      "9200 9216\n",
      "9216 9232\n",
      "9232 9248\n",
      "9248 9264\n",
      "9264 9280\n",
      "9280 9296\n",
      "9296 9312\n",
      "9312 9328\n",
      "9328 9344\n",
      "9344 9360\n",
      "9360 9376\n",
      "9376 9392\n",
      "9392 9408\n",
      "9408 9424\n",
      "9424 9440\n",
      "9440 9456\n",
      "9456 9472\n",
      "9472 9488\n",
      "9488 9504\n",
      "9504 9520\n",
      "9520 9536\n",
      "9536 9552\n",
      "9552 9568\n",
      "9568 9584\n",
      "9584 9600\n",
      "9600 9616\n",
      "9616 9632\n",
      "9632 9648\n",
      "9648 9664\n",
      "9664 9680\n",
      "9680 9696\n",
      "9696 9712\n",
      "9712 9728\n",
      "9728 9744\n",
      "9744 9760\n",
      "9760 9776\n",
      "9776 9792\n",
      "9792 9808\n",
      "9808 9824\n",
      "9824 9840\n",
      "9840 9856\n",
      "9856 9872\n",
      "9872 9888\n",
      "9888 9904\n",
      "9904 9920\n",
      "9920 9936\n",
      "9936 9952\n",
      "9952 9968\n",
      "9968 9984\n",
      "9984 10000\n",
      "10000 10016\n",
      "10016 10032\n",
      "10032 10048\n",
      "10048 10064\n",
      "10064 10080\n",
      "10080 10096\n",
      "10096 10112\n",
      "10112 10128\n",
      "10128 10144\n",
      "10144 10160\n",
      "10160 10176\n",
      "10176 10192\n",
      "10192 10208\n",
      "10208 10224\n",
      "10224 10240\n",
      "10240 10256\n",
      "10256 10272\n",
      "10272 10288\n",
      "10288 10304\n",
      "10304 10320\n",
      "10320 10336\n",
      "10336 10352\n",
      "10352 10368\n",
      "10368 10384\n",
      "10384 10400\n",
      "10400 10416\n",
      "10416 10432\n",
      "10432 10448\n",
      "10448 10464\n",
      "10464 10480\n",
      "10480 10496\n",
      "10496 10512\n",
      "10512 10528\n",
      "10528 10544\n",
      "10544 10560\n",
      "10560 10576\n",
      "10576 10592\n",
      "10592 10608\n",
      "10608 10624\n",
      "10624 10640\n",
      "10640 10656\n",
      "10656 10672\n",
      "10672 10688\n",
      "10688 10704\n",
      "10704 10720\n",
      "10720 10736\n",
      "10736 10752\n",
      "10752 10768\n",
      "10768 10784\n",
      "10784 10800\n",
      "10800 10816\n",
      "10816 10832\n",
      "10832 10848\n",
      "10848 10864\n",
      "10864 10880\n",
      "10880 10896\n",
      "10896 10912\n",
      "10912 10928\n",
      "10928 10944\n",
      "10944 10960\n",
      "10960 10976\n",
      "10976 10992\n",
      "10992 11008\n",
      "11008 11024\n",
      "11024 11040\n",
      "11040 11056\n",
      "11056 11072\n",
      "11072 11088\n",
      "11088 11104\n",
      "11104 11120\n",
      "11120 11136\n",
      "11136 11152\n",
      "11152 11168\n",
      "11168 11184\n",
      "11184 11200\n",
      "11200 11216\n",
      "11216 11232\n",
      "11232 11248\n",
      "11248 11264\n",
      "11264 11280\n",
      "11280 11296\n",
      "11296 11312\n",
      "11312 11328\n",
      "11328 11344\n",
      "11344 11360\n",
      "11360 11376\n",
      "11376 11392\n",
      "11392 11408\n",
      "11408 11424\n",
      "11424 11440\n",
      "11440 11456\n",
      "11456 11472\n",
      "11472 11488\n",
      "11488 11504\n",
      "11504 11520\n",
      "11520 11536\n",
      "11536 11552\n",
      "11552 11568\n",
      "11568 11584\n",
      "11584 11600\n",
      "11600 11616\n",
      "11616 11632\n",
      "11632 11648\n",
      "11648 11664\n",
      "11664 11680\n",
      "11680 11696\n",
      "11696 11712\n",
      "11712 11728\n",
      "11728 11744\n",
      "11744 11760\n",
      "11760 11776\n",
      "11776 11792\n",
      "11792 11808\n",
      "11808 11824\n",
      "11824 11840\n",
      "11840 11856\n",
      "11856 11872\n",
      "11872 11888\n",
      "11888 11904\n",
      "11904 11920\n",
      "11920 11936\n",
      "11936 11952\n",
      "11952 11968\n",
      "11968 11984\n",
      "11984 12000\n",
      "12000 12016\n",
      "12016 12032\n",
      "12032 12048\n",
      "12048 12064\n",
      "12064 12080\n",
      "12080 12096\n",
      "12096 12112\n",
      "12112 12128\n",
      "12128 12144\n",
      "12144 12160\n",
      "12160 12176\n",
      "12176 12192\n",
      "12192 12208\n",
      "12208 12224\n",
      "12224 12240\n",
      "12240 12256\n",
      "12256 12272\n",
      "12272 12288\n",
      "12288 12304\n",
      "12304 12320\n",
      "12320 12336\n",
      "12336 12352\n",
      "12352 12368\n",
      "12368 12384\n",
      "12384 12400\n",
      "12400 12416\n",
      "12416 12432\n",
      "12432 12448\n",
      "12448 12464\n",
      "12464 12480\n",
      "12480 12496\n",
      "12496 12512\n",
      "12512 12528\n",
      "12528 12544\n",
      "12544 12560\n",
      "12560 12576\n",
      "12576 12592\n",
      "12592 12608\n",
      "12608 12624\n",
      "12624 12640\n",
      "12640 12656\n",
      "12656 12672\n",
      "12672 12688\n",
      "12688 12704\n",
      "12704 12720\n",
      "12720 12736\n",
      "12736 12752\n",
      "12752 12768\n",
      "12768 12784\n",
      "12784 12800\n",
      "12800 12816\n",
      "12816 12832\n",
      "12832 12848\n",
      "12848 12864\n",
      "12864 12880\n",
      "12880 12896\n",
      "12896 12912\n",
      "12912 12928\n",
      "12928 12944\n",
      "12944 12960\n",
      "12960 12976\n",
      "12976 12992\n",
      "12992 13008\n",
      "13008 13024\n",
      "13024 13040\n",
      "13040 13056\n",
      "13056 13072\n",
      "13072 13088\n",
      "13088 13104\n",
      "13104 13120\n",
      "13120 13136\n",
      "13136 13152\n",
      "13152 13168\n",
      "13168 13184\n",
      "13184 13200\n",
      "13200 13216\n",
      "13216 13232\n",
      "13232 13248\n",
      "13248 13264\n",
      "13264 13280\n",
      "13280 13296\n",
      "13296 13312\n",
      "13312 13328\n",
      "13328 13344\n",
      "13344 13360\n",
      "13360 13376\n",
      "13376 13392\n",
      "13392 13408\n",
      "13408 13424\n",
      "13424 13440\n",
      "13440 13456\n",
      "13456 13472\n",
      "13472 13488\n",
      "13488 13504\n",
      "13504 13520\n",
      "13520 13536\n",
      "13536 13552\n",
      "13552 13568\n",
      "13568 13584\n",
      "13584 13600\n",
      "13600 13616\n",
      "13616 13632\n",
      "13632 13648\n",
      "13648 13664\n",
      "13664 13680\n",
      "13680 13696\n",
      "13696 13712\n",
      "13712 13728\n",
      "13728 13744\n",
      "13744 13760\n",
      "13760 13776\n",
      "13776 13792\n",
      "13792 13808\n",
      "13808 13824\n",
      "13824 13840\n",
      "13840 13856\n",
      "13856 13872\n",
      "13872 13888\n",
      "13888 13904\n",
      "13904 13920\n",
      "13920 13936\n",
      "13936 13952\n",
      "13952 13968\n",
      "13968 13984\n",
      "13984 14000\n",
      "14000 14016\n",
      "14016 14032\n",
      "14032 14048\n",
      "14048 14064\n",
      "14064 14080\n",
      "14080 14096\n",
      "14096 14112\n",
      "14112 14128\n",
      "14128 14144\n",
      "14144 14160\n",
      "14160 14176\n",
      "14176 14192\n",
      "14192 14208\n",
      "14208 14224\n",
      "14224 14240\n",
      "14240 14256\n",
      "14256 14272\n",
      "14272 14288\n",
      "14288 14304\n",
      "14304 14320\n",
      "14320 14336\n",
      "14336 14352\n",
      "14352 14368\n",
      "14368 14384\n",
      "14384 14400\n",
      "14400 14416\n",
      "14416 14432\n",
      "14432 14448\n",
      "14448 14464\n",
      "14464 14480\n",
      "14480 14496\n",
      "14496 14512\n",
      "14512 14528\n",
      "14528 14544\n",
      "14544 14560\n",
      "14560 14576\n",
      "14576 14592\n",
      "14592 14608\n",
      "14608 14624\n",
      "14624 14640\n",
      "14640 14656\n",
      "14656 14672\n",
      "14672 14688\n",
      "14688 14704\n",
      "14704 14720\n",
      "14720 14736\n",
      "14736 14752\n",
      "14752 14768\n",
      "14768 14784\n",
      "14784 14800\n",
      "14800 14816\n",
      "14816 14832\n",
      "14832 14848\n",
      "14848 14864\n",
      "14864 14880\n",
      "14880 14896\n",
      "14896 14912\n",
      "14912 14928\n",
      "14928 14944\n",
      "14944 14960\n",
      "14960 14976\n",
      "14976 14992\n",
      "14992 15008\n",
      "15008 15024\n",
      "15024 15040\n",
      "15040 15056\n",
      "15056 15072\n",
      "15072 15088\n",
      "15088 15104\n",
      "15104 15120\n",
      "15120 15136\n",
      "15136 15152\n",
      "15152 15168\n",
      "15168 15184\n",
      "15184 15200\n",
      "15200 15216\n",
      "15216 15232\n",
      "15232 15248\n",
      "15248 15264\n",
      "15264 15280\n",
      "15280 15296\n",
      "15296 15312\n",
      "15312 15328\n",
      "15328 15344\n",
      "15344 15360\n",
      "15360 15376\n",
      "15376 15392\n",
      "15392 15408\n",
      "15408 15424\n",
      "15424 15440\n",
      "15440 15456\n",
      "15456 15472\n",
      "15472 15488\n",
      "15488 15504\n",
      "15504 15520\n",
      "15520 15536\n",
      "15536 15552\n",
      "15552 15568\n",
      "15568 15584\n",
      "15584 15600\n",
      "15600 15616\n",
      "15616 15632\n",
      "15632 15648\n",
      "15648 15664\n",
      "15664 15680\n",
      "15680 15696\n",
      "15696 15712\n",
      "15712 15728\n",
      "15728 15744\n",
      "15744 15760\n",
      "15760 15776\n",
      "15776 15792\n",
      "15792 15808\n",
      "15808 15824\n",
      "15824 15840\n",
      "15840 15856\n",
      "15856 15872\n",
      "15872 15888\n",
      "15888 15904\n",
      "15904 15920\n",
      "15920 15936\n",
      "15936 15952\n",
      "15952 15968\n",
      "15968 15984\n",
      "15984 16000\n",
      "16000 16016\n",
      "16016 16032\n",
      "16032 16048\n",
      "16048 16064\n",
      "16064 16080\n",
      "16080 16096\n",
      "16096 16112\n",
      "16112 16128\n",
      "16128 16144\n",
      "16144 16160\n",
      "16160 16176\n",
      "16176 16192\n",
      "16192 16208\n",
      "16208 16224\n",
      "16224 16240\n",
      "16240 16256\n",
      "16256 16272\n",
      "16272 16288\n",
      "16288 16304\n",
      "16304 16320\n",
      "16320 16336\n",
      "16336 16352\n",
      "16352 16368\n",
      "16368 16384\n",
      "16384 16400\n",
      "16400 16416\n",
      "16416 16432\n",
      "16432 16448\n",
      "16448 16464\n",
      "16464 16480\n",
      "16480 16496\n",
      "16496 16512\n",
      "16512 16528\n",
      "16528 16544\n",
      "16544 16560\n",
      "16560 16576\n",
      "16576 16592\n",
      "16592 16608\n",
      "16608 16624\n",
      "16624 16640\n",
      "16640 16656\n",
      "16656 16672\n",
      "16672 16688\n",
      "16688 16704\n",
      "16704 16720\n",
      "16720 16736\n",
      "16736 16752\n",
      "16752 16768\n",
      "16768 16784\n",
      "16784 16800\n",
      "16800 16816\n",
      "16816 16832\n",
      "16832 16848\n",
      "16848 16864\n",
      "16864 16880\n",
      "16880 16896\n",
      "16896 16912\n",
      "16912 16928\n",
      "16928 16944\n",
      "16944 16960\n",
      "16960 16976\n",
      "16976 16992\n",
      "16992 17008\n",
      "17008 17024\n",
      "17024 17040\n",
      "17040 17056\n",
      "17056 17072\n",
      "17072 17088\n",
      "17088 17104\n",
      "17104 17120\n",
      "17120 17136\n",
      "17136 17152\n",
      "17152 17168\n",
      "17168 17184\n",
      "17184 17200\n",
      "17200 17216\n",
      "17216 17232\n",
      "17232 17248\n",
      "17248 17264\n",
      "17264 17280\n",
      "17280 17296\n",
      "17296 17312\n",
      "17312 17328\n",
      "17328 17344\n",
      "17344 17360\n",
      "17360 17376\n",
      "17376 17392\n",
      "17392 17408\n",
      "17408 17424\n",
      "17424 17440\n",
      "17440 17456\n",
      "17456 17472\n",
      "17472 17488\n",
      "17488 17504\n",
      "17504 17520\n",
      "17520 17536\n",
      "17536 17552\n",
      "17552 17568\n",
      "17568 17584\n",
      "17584 17600\n",
      "17600 17616\n",
      "17616 17632\n",
      "17632 17648\n",
      "17648 17664\n",
      "17664 17680\n",
      "17680 17696\n",
      "17696 17712\n",
      "17712 17728\n",
      "17728 17744\n",
      "17744 17760\n",
      "17760 17776\n",
      "17776 17792\n",
      "17792 17808\n",
      "17808 17824\n",
      "17824 17840\n",
      "17840 17856\n",
      "17856 17872\n",
      "17872 17888\n",
      "17888 17904\n",
      "17904 17920\n",
      "17920 17936\n",
      "17936 17952\n",
      "17952 17968\n",
      "17968 17984\n",
      "17984 18000\n",
      "18000 18016\n",
      "18016 18032\n",
      "18032 18048\n",
      "18048 18064\n",
      "18064 18080\n",
      "18080 18096\n",
      "18096 18112\n",
      "18112 18128\n",
      "18128 18144\n",
      "18144 18160\n",
      "18160 18176\n",
      "18176 18192\n",
      "18192 18208\n",
      "18208 18224\n",
      "18224 18240\n",
      "18240 18256\n",
      "18256 18272\n",
      "18272 18288\n",
      "18288 18304\n",
      "18304 18320\n",
      "18320 18336\n",
      "18336 18352\n",
      "18352 18368\n",
      "18368 18384\n",
      "18384 18400\n",
      "18400 18416\n",
      "18416 18432\n",
      "18432 18448\n",
      "18448 18464\n",
      "18464 18480\n",
      "18480 18496\n",
      "18496 18512\n",
      "18512 18528\n",
      "18528 18544\n",
      "18544 18560\n",
      "18560 18576\n",
      "18576 18592\n",
      "18592 18608\n",
      "18608 18624\n",
      "18624 18640\n",
      "18640 18656\n",
      "18656 18672\n",
      "18672 18688\n",
      "18688 18704\n",
      "18704 18720\n",
      "18720 18736\n",
      "18736 18752\n",
      "18752 18768\n",
      "18768 18784\n",
      "18784 18800\n",
      "18800 18816\n",
      "18816 18832\n",
      "18832 18848\n",
      "18848 18864\n",
      "18864 18880\n",
      "18880 18896\n",
      "18896 18912\n",
      "18912 18928\n",
      "18928 18944\n",
      "18944 18960\n",
      "18960 18976\n",
      "18976 18992\n",
      "18992 19008\n",
      "19008 19024\n",
      "19024 19040\n",
      "19040 19056\n",
      "19056 19072\n",
      "19072 19088\n",
      "19088 19104\n",
      "19104 19120\n",
      "19120 19136\n",
      "19136 19152\n",
      "19152 19168\n",
      "19168 19184\n",
      "19184 19200\n",
      "19200 19216\n",
      "19216 19232\n",
      "19232 19248\n",
      "19248 19264\n",
      "19264 19280\n",
      "19280 19296\n",
      "19296 19312\n",
      "19312 19328\n",
      "19328 19344\n",
      "19344 19360\n",
      "19360 19376\n",
      "19376 19392\n",
      "19392 19408\n",
      "19408 19424\n",
      "19424 19440\n",
      "19440 19456\n",
      "19456 19472\n",
      "19472 19488\n",
      "19488 19504\n",
      "19504 19520\n",
      "19520 19536\n",
      "19536 19552\n",
      "19552 19568\n",
      "19568 19584\n",
      "19584 19600\n",
      "19600 19616\n",
      "19616 19632\n",
      "19632 19648\n",
      "19648 19664\n",
      "19664 19680\n",
      "19680 19696\n",
      "19696 19712\n",
      "19712 19728\n",
      "19728 19744\n",
      "19744 19760\n",
      "19760 19776\n",
      "19776 19792\n",
      "19792 19808\n",
      "19808 19824\n",
      "19824 19840\n",
      "19840 19856\n",
      "19856 19872\n",
      "19872 19888\n",
      "19888 19904\n",
      "19904 19920\n",
      "19920 19936\n",
      "19936 19952\n",
      "19952 19968\n",
      "19968 19984\n",
      "19984 20000\n",
      "20000 20016\n",
      "20016 20032\n",
      "20032 20048\n",
      "20048 20064\n",
      "20064 20080\n",
      "20080 20096\n",
      "20096 20112\n",
      "20112 20128\n",
      "20128 20144\n",
      "20144 20160\n",
      "20160 20176\n",
      "20176 20192\n",
      "20192 20208\n",
      "20208 20224\n",
      "20224 20240\n",
      "20240 20256\n",
      "20256 20272\n",
      "20272 20288\n",
      "20288 20304\n",
      "20304 20320\n",
      "20320 20336\n",
      "20336 20352\n",
      "20352 20368\n",
      "20368 20384\n",
      "20384 20400\n",
      "20400 20416\n",
      "20416 20432\n",
      "20432 20448\n",
      "20448 20464\n",
      "20464 20480\n",
      "20480 20496\n",
      "20496 20512\n",
      "20512 20528\n",
      "20528 20544\n",
      "20544 20560\n",
      "20560 20576\n",
      "20576 20592\n",
      "20592 20608\n",
      "20608 20624\n",
      "20624 20640\n",
      "20640 20656\n",
      "20656 20672\n",
      "20672 20688\n",
      "20688 20704\n",
      "20704 20720\n",
      "20720 20736\n",
      "20736 20752\n",
      "20752 20768\n",
      "20768 20784\n",
      "20784 20800\n",
      "20800 20816\n",
      "20816 20832\n",
      "20832 20848\n",
      "20848 20864\n",
      "20864 20880\n",
      "20880 20896\n",
      "20896 20912\n",
      "20912 20928\n",
      "20928 20944\n",
      "20944 20960\n",
      "20960 20976\n",
      "20976 20992\n",
      "20992 21008\n",
      "21008 21024\n",
      "21024 21040\n",
      "21040 21056\n",
      "21056 21072\n",
      "21072 21088\n",
      "21088 21104\n",
      "21104 21120\n",
      "21120 21136\n",
      "21136 21152\n",
      "21152 21168\n",
      "21168 21184\n",
      "21184 21200\n",
      "21200 21216\n",
      "21216 21232\n",
      "21232 21248\n",
      "21248 21264\n",
      "21264 21280\n",
      "21280 21296\n",
      "21296 21312\n",
      "21312 21328\n",
      "21328 21344\n",
      "21344 21360\n",
      "21360 21376\n",
      "21376 21392\n",
      "21392 21408\n",
      "21408 21424\n",
      "21424 21440\n",
      "21440 21456\n",
      "21456 21472\n",
      "21472 21488\n",
      "21488 21504\n",
      "21504 21520\n",
      "21520 21536\n",
      "21536 21552\n",
      "21552 21568\n",
      "21568 21584\n",
      "21584 21600\n",
      "21600 21616\n",
      "21616 21632\n",
      "21632 21648\n",
      "21648 21664\n",
      "21664 21680\n",
      "21680 21696\n",
      "21696 21712\n",
      "21712 21728\n",
      "21728 21744\n",
      "21744 21760\n",
      "21760 21776\n",
      "21776 21792\n",
      "21792 21808\n",
      "21808 21824\n",
      "21824 21840\n",
      "21840 21856\n",
      "21856 21872\n",
      "21872 21888\n",
      "21888 21904\n",
      "21904 21920\n",
      "21920 21936\n",
      "21936 21952\n",
      "21952 21968\n",
      "21968 21984\n",
      "21984 22000\n",
      "22000 22016\n",
      "22016 22032\n",
      "22032 22048\n",
      "22048 22064\n",
      "22064 22080\n",
      "22080 22096\n",
      "22096 22112\n",
      "22112 22128\n",
      "22128 22144\n",
      "22144 22160\n",
      "22160 22176\n",
      "22176 22192\n",
      "22192 22208\n",
      "22208 22224\n",
      "22224 22240\n",
      "22240 22256\n",
      "22256 22272\n",
      "22272 22288\n",
      "22288 22304\n",
      "22304 22320\n",
      "22320 22336\n",
      "22336 22352\n",
      "22352 22368\n",
      "22368 22384\n",
      "22384 22400\n",
      "22400 22416\n",
      "22416 22432\n",
      "22432 22448\n",
      "22448 22464\n",
      "22464 22480\n",
      "22480 22496\n",
      "22496 22512\n",
      "22512 22528\n",
      "22528 22544\n",
      "22544 22560\n",
      "22560 22576\n",
      "22576 22592\n",
      "22592 22608\n",
      "22608 22624\n",
      "22624 22640\n",
      "22640 22656\n",
      "22656 22672\n",
      "22672 22688\n",
      "22688 22704\n",
      "22704 22720\n",
      "22720 22736\n",
      "22736 22752\n",
      "22752 22768\n",
      "22768 22784\n",
      "22784 22800\n",
      "22800 22816\n",
      "22816 22832\n",
      "22832 22848\n",
      "22848 22864\n",
      "22864 22880\n",
      "22880 22896\n",
      "22896 22912\n",
      "22912 22928\n",
      "22928 22944\n",
      "22944 22960\n",
      "22960 22976\n",
      "22976 22992\n",
      "22992 23008\n",
      "23008 23024\n",
      "23024 23040\n",
      "23040 23056\n",
      "23056 23072\n",
      "23072 23088\n",
      "23088 23104\n",
      "23104 23120\n",
      "23120 23136\n",
      "23136 23152\n",
      "23152 23168\n",
      "23168 23184\n",
      "23184 23200\n",
      "23200 23216\n",
      "23216 23232\n",
      "23232 23248\n",
      "23248 23264\n",
      "23264 23280\n",
      "23280 23296\n",
      "23296 23312\n",
      "23312 23328\n",
      "23328 23344\n",
      "23344 23360\n",
      "23360 23376\n",
      "23376 23392\n",
      "23392 23408\n",
      "23408 23424\n",
      "23424 23440\n",
      "23440 23456\n",
      "23456 23472\n",
      "23472 23488\n",
      "23488 23504\n",
      "23504 23520\n",
      "23520 23536\n",
      "23536 23552\n",
      "23552 23568\n",
      "23568 23584\n",
      "23584 23600\n",
      "23600 23616\n",
      "23616 23632\n",
      "23632 23648\n",
      "23648 23664\n",
      "23664 23680\n",
      "23680 23696\n",
      "23696 23712\n",
      "23712 23728\n",
      "23728 23744\n",
      "23744 23760\n",
      "23760 23776\n",
      "23776 23792\n",
      "23792 23808\n",
      "23808 23824\n",
      "23824 23840\n",
      "23840 23856\n",
      "23856 23872\n",
      "23872 23888\n",
      "23888 23904\n",
      "23904 23920\n",
      "23920 23936\n",
      "23936 23952\n",
      "23952 23968\n",
      "23968 23984\n",
      "23984 24000\n",
      "24000 24016\n",
      "24016 24032\n",
      "24032 24048\n",
      "24048 24064\n",
      "24064 24080\n",
      "24080 24096\n",
      "24096 24112\n",
      "24112 24128\n",
      "24128 24144\n",
      "24144 24160\n",
      "24160 24176\n",
      "24176 24192\n",
      "24192 24208\n",
      "24208 24224\n",
      "24224 24240\n",
      "24240 24256\n",
      "24256 24272\n",
      "24272 24288\n",
      "24288 24304\n",
      "24304 24320\n",
      "24320 24336\n",
      "24336 24352\n",
      "24352 24368\n",
      "24368 24384\n",
      "24384 24400\n",
      "24400 24416\n",
      "24416 24432\n",
      "24432 24448\n",
      "24448 24464\n",
      "24464 24480\n",
      "24480 24496\n",
      "24496 24512\n",
      "24512 24528\n",
      "24528 24544\n",
      "24544 24560\n",
      "24560 24576\n",
      "24576 24592\n",
      "24592 24608\n",
      "24608 24624\n",
      "24624 24640\n",
      "24640 24656\n",
      "24656 24672\n",
      "24672 24688\n",
      "24688 24704\n",
      "24704 24720\n",
      "24720 24736\n",
      "24736 24752\n",
      "24752 24768\n",
      "24768 24784\n",
      "24784 24800\n",
      "24800 24816\n",
      "24816 24832\n",
      "24832 24848\n",
      "24848 24864\n",
      "24864 24880\n",
      "24880 24896\n",
      "24896 24912\n",
      "24912 24928\n",
      "24928 24944\n",
      "24944 24960\n",
      "24960 24976\n",
      "24976 24992\n",
      "24992 25008\n",
      "25008 25024\n",
      "25024 25040\n",
      "25040 25056\n",
      "25056 25072\n",
      "25072 25088\n",
      "25088 25104\n",
      "25104 25120\n",
      "25120 25136\n",
      "25136 25152\n",
      "25152 25168\n",
      "25168 25184\n",
      "25184 25200\n",
      "25200 25216\n",
      "25216 25232\n",
      "25232 25248\n",
      "25248 25264\n",
      "25264 25280\n",
      "25280 25296\n",
      "25296 25312\n",
      "25312 25328\n",
      "25328 25344\n",
      "25344 25360\n",
      "25360 25376\n",
      "25376 25392\n",
      "25392 25408\n",
      "25408 25424\n",
      "25424 25440\n",
      "25440 25456\n",
      "25456 25472\n",
      "25472 25488\n",
      "25488 25504\n",
      "25504 25520\n",
      "25520 25536\n",
      "25536 25552\n",
      "25552 25568\n",
      "25568 25584\n",
      "25584 25600\n",
      "25600 25616\n",
      "25616 25632\n",
      "25632 25648\n",
      "25648 25664\n",
      "25664 25680\n",
      "25680 25696\n",
      "25696 25712\n",
      "25712 25728\n",
      "25728 25744\n",
      "25744 25760\n",
      "25760 25776\n",
      "25776 25792\n",
      "25792 25808\n",
      "25808 25824\n",
      "25824 25840\n",
      "25840 25856\n",
      "25856 25872\n",
      "25872 25888\n",
      "25888 25904\n",
      "25904 25920\n",
      "25920 25936\n",
      "25936 25952\n",
      "25952 25968\n",
      "25968 25984\n",
      "25984 26000\n",
      "26000 26016\n",
      "26016 26032\n",
      "26032 26048\n",
      "26048 26064\n",
      "26064 26080\n",
      "26080 26096\n",
      "26096 26112\n",
      "26112 26128\n",
      "26128 26144\n",
      "26144 26160\n",
      "26160 26176\n",
      "26176 26192\n",
      "26192 26208\n",
      "26208 26224\n",
      "26224 26240\n",
      "26240 26256\n",
      "26256 26272\n",
      "26272 26288\n",
      "26288 26304\n",
      "26304 26320\n",
      "26320 26336\n",
      "26336 26352\n",
      "26352 26368\n",
      "26368 26384\n",
      "26384 26400\n",
      "26400 26416\n",
      "26416 26432\n",
      "26432 26448\n",
      "26448 26464\n",
      "26464 26480\n",
      "26480 26496\n",
      "26496 26512\n",
      "26512 26528\n",
      "26528 26544\n",
      "26544 26560\n",
      "26560 26576\n",
      "26576 26592\n",
      "26592 26608\n",
      "26608 26624\n",
      "26624 26640\n",
      "26640 26656\n",
      "26656 26672\n",
      "26672 26688\n",
      "26688 26704\n",
      "26704 26720\n",
      "26720 26736\n",
      "26736 26752\n",
      "26752 26768\n",
      "26768 26784\n",
      "26784 26800\n",
      "26800 26816\n",
      "26816 26832\n",
      "26832 26848\n",
      "26848 26864\n",
      "26864 26880\n",
      "26880 26896\n",
      "26896 26912\n",
      "26912 26928\n",
      "26928 26944\n",
      "26944 26960\n",
      "26960 26976\n",
      "26976 26992\n",
      "26992 27008\n",
      "27008 27024\n",
      "27024 27040\n",
      "27040 27056\n",
      "27056 27072\n",
      "27072 27088\n",
      "27088 27104\n",
      "27104 27120\n",
      "27120 27136\n",
      "27136 27152\n",
      "27152 27168\n",
      "27168 27184\n",
      "27184 27200\n",
      "27200 27216\n",
      "27216 27232\n",
      "27232 27248\n",
      "27248 27264\n",
      "27264 27280\n",
      "27280 27296\n",
      "27296 27312\n",
      "27312 27328\n",
      "27328 27344\n",
      "27344 27360\n",
      "27360 27376\n",
      "27376 27392\n",
      "27392 27408\n",
      "27408 27424\n",
      "27424 27440\n",
      "27440 27456\n",
      "27456 27472\n",
      "27472 27488\n",
      "27488 27504\n",
      "27504 27520\n",
      "27520 27536\n",
      "27536 27552\n",
      "27552 27568\n",
      "27568 27584\n",
      "27584 27600\n",
      "27600 27616\n",
      "27616 27632\n",
      "27632 27648\n",
      "27648 27664\n",
      "27664 27680\n",
      "27680 27696\n",
      "27696 27712\n",
      "27712 27728\n",
      "27728 27744\n",
      "27744 27760\n",
      "27760 27776\n",
      "27776 27792\n",
      "27792 27808\n",
      "27808 27824\n",
      "27824 27840\n",
      "27840 27856\n",
      "27856 27872\n",
      "27872 27888\n",
      "27888 27904\n",
      "27904 27920\n",
      "27920 27936\n",
      "27936 27952\n",
      "27952 27968\n",
      "27968 27984\n",
      "27984 28000\n",
      "28000 28016\n",
      "28016 28032\n",
      "28032 28048\n",
      "28048 28064\n",
      "28064 28080\n",
      "28080 28096\n",
      "28096 28112\n",
      "28112 28128\n",
      "28128 28144\n",
      "28144 28160\n",
      "28160 28176\n",
      "28176 28192\n",
      "28192 28208\n",
      "28208 28224\n",
      "28224 28240\n",
      "28240 28256\n",
      "28256 28272\n",
      "28272 28288\n",
      "28288 28304\n",
      "28304 28320\n",
      "28320 28336\n",
      "28336 28352\n",
      "28352 28368\n",
      "28368 28384\n",
      "28384 28400\n",
      "28400 28416\n",
      "28416 28432\n",
      "28432 28448\n",
      "28448 28464\n",
      "28464 28480\n",
      "28480 28496\n",
      "28496 28512\n",
      "28512 28528\n",
      "28528 28544\n",
      "28544 28560\n",
      "28560 28576\n",
      "28576 28592\n",
      "28592 28608\n",
      "28608 28624\n",
      "28624 28640\n",
      "28640 28656\n",
      "28656 28672\n",
      "28672 28688\n",
      "28688 28704\n",
      "28704 28720\n",
      "28720 28736\n",
      "28736 28752\n",
      "28752 28768\n",
      "28768 28784\n",
      "28784 28800\n",
      "28800 28816\n",
      "28816 28832\n",
      "28832 28848\n",
      "28848 28864\n",
      "28864 28880\n",
      "28880 28896\n",
      "28896 28912\n",
      "28912 28928\n",
      "28928 28944\n",
      "28944 28960\n",
      "28960 28976\n",
      "28976 28992\n",
      "28992 29008\n",
      "29008 29024\n",
      "29024 29040\n",
      "29040 29056\n",
      "29056 29072\n",
      "29072 29088\n",
      "29088 29104\n",
      "29104 29120\n",
      "29120 29136\n",
      "29136 29152\n",
      "29152 29168\n",
      "29168 29184\n",
      "29184 29200\n",
      "29200 29216\n",
      "29216 29232\n",
      "29232 29248\n",
      "29248 29264\n",
      "29264 29280\n",
      "29280 29296\n",
      "29296 29312\n",
      "29312 29328\n",
      "29328 29344\n",
      "29344 29360\n",
      "29360 29376\n",
      "29376 29392\n",
      "29392 29408\n",
      "29408 29424\n",
      "29424 29440\n",
      "29440 29456\n",
      "29456 29472\n",
      "29472 29488\n",
      "29488 29504\n",
      "29504 29520\n",
      "29520 29536\n",
      "29536 29552\n",
      "29552 29568\n",
      "29568 29584\n",
      "29584 29600\n",
      "29600 29616\n",
      "29616 29632\n",
      "29632 29648\n",
      "29648 29664\n",
      "29664 29680\n",
      "29680 29696\n",
      "29696 29712\n",
      "29712 29728\n",
      "29728 29744\n",
      "29744 29760\n",
      "29760 29776\n",
      "29776 29792\n",
      "29792 29808\n",
      "29808 29824\n",
      "29824 29840\n",
      "29840 29856\n",
      "29856 29872\n",
      "29872 29888\n",
      "29888 29904\n",
      "29904 29920\n",
      "29920 29936\n",
      "29936 29952\n",
      "29952 29968\n",
      "29968 29984\n",
      "29984 30000\n",
      "30000 30016\n",
      "30016 30032\n",
      "30032 30048\n",
      "30048 30064\n",
      "30064 30080\n",
      "30080 30096\n",
      "30096 30112\n",
      "30112 30128\n",
      "30128 30144\n",
      "30144 30160\n",
      "30160 30176\n",
      "30176 30192\n",
      "30192 30208\n",
      "30208 30224\n",
      "30224 30240\n",
      "30240 30256\n",
      "30256 30272\n",
      "30272 30288\n",
      "30288 30304\n",
      "30304 30320\n",
      "30320 30336\n",
      "30336 30352\n",
      "30352 30368\n",
      "30368 30384\n",
      "30384 30400\n",
      "30400 30416\n",
      "30416 30432\n",
      "30432 30448\n",
      "30448 30464\n",
      "30464 30480\n",
      "30480 30496\n",
      "30496 30512\n",
      "30512 30528\n",
      "30528 30544\n",
      "30544 30560\n",
      "30560 30576\n",
      "30576 30592\n",
      "30592 30608\n",
      "30608 30624\n",
      "30624 30640\n",
      "30640 30656\n",
      "30656 30672\n",
      "30672 30688\n",
      "30688 30704\n",
      "30704 30720\n",
      "30720 30736\n",
      "30736 30752\n",
      "30752 30768\n",
      "30768 30784\n",
      "30784 30800\n",
      "30800 30816\n",
      "30816 30832\n",
      "30832 30848\n",
      "30848 30864\n",
      "30864 30880\n",
      "30880 30896\n",
      "30896 30912\n",
      "30912 30928\n",
      "30928 30944\n",
      "30944 30960\n",
      "30960 30976\n",
      "30976 30992\n",
      "30992 31008\n",
      "31008 31024\n",
      "31024 31040\n",
      "31040 31056\n",
      "31056 31072\n",
      "31072 31088\n",
      "31088 31104\n",
      "31104 31120\n",
      "31120 31136\n",
      "31136 31152\n",
      "31152 31168\n",
      "31168 31184\n",
      "31184 31200\n",
      "31200 31216\n",
      "31216 31232\n",
      "31232 31248\n",
      "31248 31264\n",
      "31264 31280\n",
      "31280 31296\n",
      "31296 31312\n",
      "31312 31328\n",
      "31328 31344\n",
      "31344 31360\n",
      "31360 31376\n",
      "31376 31392\n",
      "31392 31408\n",
      "31408 31424\n",
      "31424 31440\n",
      "31440 31456\n",
      "31456 31472\n",
      "31472 31488\n",
      "31488 31504\n",
      "31504 31520\n",
      "31520 31536\n",
      "31536 31552\n",
      "31552 31568\n",
      "31568 31584\n",
      "31584 31600\n",
      "31600 31616\n",
      "31616 31632\n",
      "31632 31648\n",
      "31648 31664\n",
      "31664 31680\n",
      "31680 31696\n",
      "31696 31712\n",
      "31712 31728\n",
      "31728 31744\n",
      "31744 31760\n",
      "31760 31776\n",
      "31776 31792\n",
      "31792 31808\n",
      "31808 31824\n",
      "31824 31840\n",
      "31840 31856\n",
      "31856 31872\n",
      "31872 31888\n",
      "31888 31904\n",
      "31904 31920\n",
      "31920 31936\n",
      "31936 31952\n",
      "31952 31968\n",
      "31968 31984\n",
      "31984 32000\n",
      "32000 32016\n",
      "32016 32032\n",
      "32032 32048\n",
      "32048 32064\n",
      "32064 32080\n",
      "32080 32096\n",
      "32096 32112\n",
      "32112 32128\n",
      "32128 32144\n",
      "32144 32160\n",
      "32160 32176\n",
      "32176 32192\n",
      "32192 32208\n",
      "32208 32224\n",
      "32224 32240\n",
      "32240 32256\n",
      "32256 32272\n",
      "32272 32288\n",
      "32288 32304\n",
      "32304 32320\n",
      "32320 32336\n",
      "32336 32352\n",
      "32352 32368\n",
      "32368 32384\n",
      "32384 32400\n",
      "32400 32416\n",
      "32416 32432\n",
      "32432 32448\n",
      "32448 32464\n",
      "32464 32480\n",
      "32480 32496\n",
      "32496 32512\n",
      "32512 32528\n",
      "32528 32544\n",
      "32544 32560\n",
      "32560 32576\n",
      "32576 32592\n",
      "32592 32608\n",
      "32608 32624\n",
      "32624 32640\n",
      "32640 32656\n",
      "32656 32672\n",
      "32672 32688\n",
      "32688 32704\n",
      "32704 32720\n",
      "32720 32736\n",
      "32736 32752\n",
      "32752 32768\n",
      "32768 32784\n",
      "32784 32800\n",
      "32800 32816\n",
      "32816 32832\n",
      "32832 32848\n",
      "32848 32864\n",
      "32864 32880\n",
      "32880 32896\n",
      "32896 32912\n",
      "32912 32928\n",
      "32928 32944\n",
      "32944 32960\n",
      "32960 32976\n",
      "32976 32992\n",
      "32992 33008\n",
      "33008 33024\n",
      "33024 33040\n",
      "33040 33056\n",
      "33056 33072\n",
      "33072 33088\n",
      "33088 33104\n",
      "33104 33120\n",
      "33120 33136\n",
      "33136 33152\n",
      "33152 33168\n",
      "33168 33184\n",
      "33184 33200\n",
      "33200 33216\n",
      "33216 33232\n",
      "33232 33248\n",
      "33248 33264\n",
      "33264 33280\n",
      "33280 33296\n",
      "33296 33312\n",
      "33312 33328\n",
      "33328 33344\n",
      "33344 33360\n",
      "33360 33376\n",
      "33376 33392\n",
      "33392 33408\n",
      "33408 33424\n",
      "33424 33440\n",
      "33440 33456\n",
      "33456 33472\n",
      "33472 33488\n",
      "33488 33504\n",
      "33504 33520\n",
      "33520 33536\n",
      "33536 33552\n",
      "33552 33568\n",
      "33568 33584\n",
      "33584 33600\n",
      "33600 33616\n",
      "33616 33632\n",
      "33632 33648\n",
      "33648 33664\n",
      "33664 33680\n",
      "33680 33696\n",
      "33696 33712\n",
      "33712 33728\n",
      "33728 33744\n",
      "33744 33760\n",
      "33760 33776\n",
      "33776 33792\n",
      "33792 33808\n",
      "33808 33824\n",
      "33824 33840\n",
      "33840 33856\n",
      "33856 33872\n",
      "33872 33888\n",
      "33888 33904\n",
      "33904 33920\n",
      "33920 33936\n",
      "33936 33952\n",
      "33952 33968\n",
      "33968 33984\n",
      "33984 34000\n",
      "34000 34016\n",
      "34016 34032\n",
      "34032 34048\n",
      "34048 34064\n",
      "34064 34080\n",
      "34080 34096\n",
      "34096 34112\n",
      "34112 34128\n",
      "34128 34144\n",
      "34144 34160\n",
      "34160 34176\n",
      "34176 34192\n",
      "34192 34208\n",
      "34208 34224\n",
      "34224 34240\n",
      "34240 34256\n",
      "34256 34272\n",
      "34272 34288\n",
      "34288 34304\n",
      "34304 34320\n",
      "34320 34336\n",
      "34336 34352\n",
      "34352 34368\n",
      "34368 34384\n",
      "34384 34400\n",
      "34400 34416\n",
      "34416 34432\n",
      "34432 34448\n",
      "34448 34464\n",
      "34464 34480\n",
      "34480 34496\n",
      "34496 34512\n",
      "34512 34528\n",
      "34528 34544\n",
      "34544 34560\n",
      "34560 34576\n",
      "34576 34592\n",
      "34592 34608\n",
      "34608 34624\n",
      "34624 34640\n",
      "34640 34656\n",
      "34656 34672\n",
      "34672 34687\n"
     ]
    }
   ],
   "source": [
    "image_test = ds['test']['image']\n",
    "\n",
    "# %%\n",
    "\n",
    "output_dicttest = []\n",
    "for i in range(0, len(image_test),16):\n",
    "    k = min(i+16, len(image_test)-1)\n",
    "    print(i,k)\n",
    "    output_dicttest.append(classifier(image_test[i:k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict_test = []\n",
    "for i in output_dicttest: \n",
    "    output_dict_test += i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "target = []\n",
    "\n",
    "for i in output_dict_test:\n",
    "    for ii in i:\n",
    "        # A = [0,0]\n",
    "        if ii['label'] =='1':\n",
    "            A = [1-ii['score'],ii['score']]\n",
    "        # else:\n",
    "            # A[0] = ii['score']\n",
    "    score.append(A)\n",
    "label = ds['test']['label']\n",
    "for i in label:\n",
    "    if i == 0:\n",
    "        target.append([1,0])\n",
    "    if i == 1:\n",
    "        target.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34687, 2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.Tensor(score)\n",
    "target = torch.Tensor(target)\n",
    "label= torch.Tensor(label)[:-1]\n",
    "test_loss=torch.nn.functional.cross_entropy(score, target)\n",
    "test_loss_bin= torch.nn.functional.binary_cross_entropy(score[:,1], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6256), tensor(1.7951))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss,test_loss_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acc slide-wise\n",
    "(Ideally it should be patient-wise, but I did not match slide ID back to patient ID. I do not have the complete patient-slide list either.\n",
    "So the train/test split was based on slide, but not patient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[39mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 4 columns passed, passed data had 3 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m label, score \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold2 ,score\n\u001b[1;32m     20\u001b[0m \u001b[39m# df_train_result = cal_loss_df(df_train)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m df_val_result \u001b[39m=\u001b[39m cal_loss_df(df_val)\n",
      "Cell \u001b[0;32mIn[72], line 7\u001b[0m, in \u001b[0;36mcal_loss_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     df_pid\u001b[39m=\u001b[39mdf[df[\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m pid]\n\u001b[1;32m      6\u001b[0m     label,pred, score \u001b[39m=\u001b[39m cal_loss_pid(df_pid)\n\u001b[0;32m----> 7\u001b[0m     pd\u001b[39m.\u001b[39mconcat([result_df, pd\u001b[39m.\u001b[39mDataFrame([[pid, pred, score]],columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mslide_ID\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m],)])\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m result_df\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 806\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    807\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    808\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         data,\n\u001b[1;32m    810\u001b[0m         columns,\n\u001b[1;32m    811\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         dtype,\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    814\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    815\u001b[0m         arrays,\n\u001b[1;32m    816\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/miniconda3/envs/NA/lib/python3.11/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[39m=\u001b[39m convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 4 columns passed, passed data had 3 columns"
     ]
    }
   ],
   "source": [
    "def cal_loss_df(df):\n",
    "    result_df = pd.DataFrame(None,columns=['slide_ID', 'label','pred','score'],)\n",
    "    pids = df['ID'].unique()\n",
    "    for pid in pids:\n",
    "        df_pid=df[df['ID'] == pid]\n",
    "        label,pred, score = cal_loss_pid(df_pid)\n",
    "        pd.concat([result_df, pd.DataFrame([[pid, pred, score]],columns=['slide_ID','label', 'pred','score'],)])\n",
    "    return result_df\n",
    "    \n",
    "\n",
    "def cal_loss_pid(df, mode = 'majority'):\n",
    "    assert len(df['Y'].unique()) ==1\n",
    "    label = df['Y']\n",
    "    if mode == 'majority':\n",
    "        score = sum(df['pred'])/len(df)\n",
    "    elif mode == 'mean':\n",
    "        score = df['score'].mean()\n",
    "    return label, score >= threshold2 ,score\n",
    "\n",
    "# df_train_result = cal_loss_df(df_train)\n",
    "df_val_result = cal_loss_df(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Acc train: ' ,accuracy_score(df_train_result['label'],df_train_result['pred'])\n",
    "# print('AUC train: ' ,auc(df_train_result['label'],df_train_result['score'])\n",
    "# print('AUPRC train: ' ,precision_score(df_train_result['label'],df_train_result['score'])\n",
    "print(\"\")\n",
    "print('Acc test: ' ,accuracy_score(df_test_result['label'],df_test_result['pred'])\n",
    "print('AUC test: ' ,auc(df_test_result['label'],df_test_result['score'])\n",
    "print('AUPRC test: ' ,precision_score(df_test_result['label'],df_test_result['score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad6a615a1ef1d8e0343a28e4a9fbe60c0c10bbf6792d1c88d31f83b0898db28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
